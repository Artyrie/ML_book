{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Artyrie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed: 41.9min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 55.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 매개변수 조합 : {'clf__C': 100.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000001C80A0616A8>}\n",
      "CV 정확도 : 0.888\n",
      "테스트 정확도 : 0.890 \n"
     ]
    }
   ],
   "source": [
    "\"\"\" Ch8_03 \"\"\"\n",
    "\n",
    "\"\"\" 영화 리뷰 불러오기 \"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding = 'utf-8')\n",
    "df.head(5)\n",
    "\n",
    "\"\"\" 함수 정의 및 데이터 분할 \"\"\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "porter = PorterStemmer()\n",
    "tfidf = TfidfTransformer(use_idf = True, norm = 'l2', smooth_idf = True)\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text) :\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text) : \n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "\"\"\" 5겹 교차 검증으로 로지스틱 회귀 매개변수 조합 찾기 \"\"\"\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(solver='liblinear', random_state=0))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5, verbose=1, n_jobs=-1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "print('최적의 매개변수 조합 : %s' % gs_lr_tfidf.best_params_)\n",
    "\n",
    "print('CV 정확도 : %.3f' % gs_lr_tfidf.best_score_)\n",
    "\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('테스트 정확도 : %.3f ' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log', max_iter=1, random_state=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Ch8_04 \"\"\"\n",
    "\n",
    "\"\"\" 데이터 정제 및 토큰화 \"\"\"\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text) : \n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "\"\"\" 문서를 하나씩 읽어서 반환 \"\"\"\n",
    "\n",
    "def stream_docs(path) :\n",
    "    with open(path, 'r', encoding = 'utf-8') as csv : \n",
    "        next(csv)\n",
    "        # 헤더 넘기기\n",
    "        for line in csv : \n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "\n",
    "next(stream_docs(path = 'movie_data.csv'))\n",
    "\n",
    "\"\"\" 지정한 만큼 문서를 반환 \"\"\"\n",
    "\n",
    "def get_minibatch(doc_stream, size) : \n",
    "    docs, y = [], []\n",
    "    try : \n",
    "        for _ in range(size) : \n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration : \n",
    "        pass\n",
    "    return docs, y\n",
    "\n",
    "\"\"\" 데이터 종류에 상관없는 HashingVectorizer\"\"\"\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error = 'ignore',\n",
    "                        n_features = 2**21,\n",
    "                         # 해시 충돌 가능성을 줄임 => 로지스틱 회귀 모델의 가중치 개수도 늘어남\n",
    "                        preprocessor = None,\n",
    "                        tokenizer = tokenizer)\n",
    "clf = SGDClassifier(loss = 'log', random_state = 1, max_iter = 1)\n",
    "doc_stream = stream_docs(path = 'movie_data.csv')\n",
    "\n",
    "\"\"\" 외부 메모리 학습 \"\"\"\n",
    "import pyprind\n",
    "\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "for _ in range(45) : \n",
    "    X_train, y_train = get_minibatch(doc_stream, size = 1000)\n",
    "    if not X_train : \n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes = classes)\n",
    "    pbar.update()\n",
    "    \n",
    "\"\"\" 모델 테스트 하기 \"\"\"\n",
    "\n",
    "X_test, y_test = get_minibatch(doc_stream, size = 5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('정확도: %.3f' % clf.score(X_test, y_test))\n",
    "\n",
    "clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 어플에 쓰일 파일과 데이터 저장 \"\"\"\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "\n",
    "if not os.path.exists(dest) : \n",
    "    os.makedirs(dest)\n",
    "\n",
    "pickle.dump(stop,\n",
    "           open(os.path.join(dest, 'stopwords.pkl'), 'wb'),\n",
    "           protocol = 4)\n",
    "pickle.dump(clf,\n",
    "           open(os.path.join(dest, 'classifier.pkl'), 'wb'),\n",
    "            # wb 매개변수로 이진 모드로 파일을 염 (pickle 때문)\n",
    "           protocol = 4)\n",
    "            # protocol 4 지정시 3.4 버전 이상에서 사용 가능\n",
    "\n",
    "# HashingVectorizer는 학습 과정이 없어 직렬화가 필요 없다.\n",
    "# 대신 객체를 임포트할 수 있도록 파이썬 스크립트가 필요하다.\n",
    "# 해당 파일은 vectorizer.py 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
