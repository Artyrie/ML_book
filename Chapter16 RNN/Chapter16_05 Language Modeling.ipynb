{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162850\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "import numpy as np\n",
    "\n",
    "with open('../Dataset/pg2265.txt', 'r', encoding = 'utf-8') as f :\n",
    "    text = f.read()\n",
    "\n",
    "text = text[16247:] # 구텐베르그 페이지의 파일이 수정됨.\n",
    "chars = set(text)\n",
    "char2int = {ch:i for i, ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text], dtype = np.int32)\n",
    "\n",
    "print(len(text))\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(sequence, batch_size, num_steps) : \n",
    "    mini_batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence) / mini_batch_length)\n",
    "    if num_batches * mini_batch_length + 1 > len(sequence) : \n",
    "        num_batches = num_batches - 1\n",
    "    # 전체 배치에 포함되지 않는 시퀀스 끝부분은 삭제\n",
    "    x = sequence[0: num_batches * mini_batch_length]\n",
    "    y = sequence[1: num_batches * mini_batch_length + 1]\n",
    "    # x와 y를 시퀀스 배치의 리스트로 나눔\n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    # 합침 (size = batch_size * mini_batch_length)\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2540)\n",
      "[15 31 44 38 15 47 30  2 44 53]\n",
      "[31 44 38 15 47 30  2 44 53 29]\n",
      "The Traged\n",
      "he Tragedi\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "train_x, train_y = reshape_data(text_ints, 64, 10)\n",
    "print(train_x.shape)\n",
    "print(train_x[0, :10])\n",
    "print(train_y[0, :10])\n",
    "print(''.join(int2char[i] for i in train_x[0, :10]))\n",
    "print(''.join(int2char[i] for i in train_y[0, :10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_generator(data_x, data_y, num_steps) : \n",
    "    batch_size, tot_batch_length = data_x.shape[0:2]\n",
    "    num_batches = int(tot_batch_length / num_steps)\n",
    "    \n",
    "    for b in range(num_batches) : \n",
    "        yield (data_x[:, b * num_steps : (b + 1) * num_steps],\n",
    "              data_y[:, b * num_steps : (b + 1) * num_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 15) (64, 15) The Tragedie of   he Tragedie of \n",
      "(64, 15) (64, 15)  Hamlet**Actus    Hamlet**Actus P\n",
      "(64, 15) (64, 15) Primus. Scoena    rimus. Scoena P\n",
      "(64, 15) (64, 15) Prima.**Enter B   rima.**Enter Ba\n",
      "(64, 15) (64, 15) arnardo and Fra   rnardo and Fran\n",
      "(64, 15) (64, 15) ncisco two Cent   cisco two Centi\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "bgen = create_batch_generator(train_x[:, :100], train_y[:, :100], 15)\n",
    "\n",
    "for x, y in bgen : \n",
    "    print(x.shape, y.shape, end = ' ')\n",
    "    print(''.join(int2char[i] for i in x[0, :]).replace('\\n', '*'), ' ',\n",
    "         ''.join(int2char[i] for i in y[0, :]).replace('\\n', '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2500) (64, 2500)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100\n",
    "train_x, train_y = reshape_data(text_ints, batch_size, num_steps)\n",
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2500, 65) (64, 2500, 65)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_encoded_x = to_categorical(train_x) # 원-핫 인코딩된 벡터로 변환\n",
    "train_encoded_y = to_categorical(train_y)\n",
    "print(train_encoded_x.shape, train_encoded_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64\n"
     ]
    }
   ],
   "source": [
    "print(np.max(train_x), np.max(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "char_model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(chars) # 텍스트에 있는 모든 글자 수\n",
    "char_model.add(layers.LSTM(128, input_shape = (None, num_classes), return_sequences = True))\n",
    "# 가변 길이 시퀀스를 처리하기 위해 Input이 (None, 원-핫 인코딩 벡터 크기)\n",
    "# 샘플링 : batch = 1, num_steps = 1 // 훈련 : batch = 64, num_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_model.add(layers.TimeDistributed(layers.Dense(num_classes, activation = 'softmax')))\n",
    "# 모든 타임 스텝에 대한 손실을 계산해야 하므로 LSTM의 3차원 텐서를 다루어야 함.\n",
    "# 따라서 Flatten을 넣지 않고 LSTM 층의 출력을 타임 스텝 순으로 Dense 층에 주입하고 결과를 받아\n",
    "# 다시 타임 스텝 순으로 쌓도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, None, 128)         99328     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 65)          8385      \n",
      "=================================================================\n",
      "Total params: 107,713\n",
      "Trainable params: 107,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "char_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "adam = Adam(clipnorm = 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_model.compile(loss = 'categorical_crossentropy', optimizer = adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500) : \n",
    "    bgen = create_batch_generator(train_encoded_x, train_encoded_y, num_steps)\n",
    "    char_model.fit_generator(bgen, steps_per_epoch = 25, epochs = 1,\n",
    "                            verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def get_top_char(probas, char_size, top_n = 5) :\n",
    "    p = np.squeeze(probas)\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    ch_id = np.random.choice(char_size, 1, p = p)[0]\n",
    "    return ch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"The \"\n",
    "\n",
    "for ch in seed_text : \n",
    "    num = [char2int[ch]]\n",
    "    onehot = to_categorical(num, num_classes = 65)\n",
    "    onehot = np.expand_dims(onehot, axis = 0)\n",
    "    probas = char_model.predict(onehot)\n",
    "\n",
    "num = get_top_char(probas, len(chars))\n",
    "seed_text += int2char[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The wene thes teroush wings anges, there t wingrono aterer  this we wat tenghend thies thoner t men as, t waro mauster  mar me th me, withe anode athin the the me m. m.  witonge man  thill me ather winder thes, mus me mat mes thour  me ast wingrer te    wie tod auero  menourere anende winge wau  windongh ande w wit,\n",
      "\n",
      " with me  m. wis thit m. asth angs arsthe,  as and   thithe,  t winers winde w wer me,\n",
      "  we t ther m. where, to  wit au mursthe w   m. m.  thinghind  wie  te,\n",
      "  tod me merther ath wauren\n"
     ]
    }
   ],
   "source": [
    "# 500번 반복해보기\n",
    "\n",
    "for i in range(500) : \n",
    "    onehot = to_categorical([num], num_classes = 65)\n",
    "    onehot = np.expand_dims(onehot, axis = 0)\n",
    "    probas = char_model.predict(onehot)\n",
    "    num = get_top_char(probas, len(chars))\n",
    "    seed_text += int2char[num]\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
