{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 데이터 정제 및 토큰화 \"\"\"\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text) : \n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"\\'Tycus\\' is almost as bad as a science fiction film can go.<br /><br />I can hardly find something good to say about this film. The premises are completely wrong. A comet is supposed to hit the Moon and cause catastrophic damage to Earth, but nobody believes the scientist who predicts this.A whole underground city plus a launching pad for nuclear armed rockets is build in the California mountains without anybody noticing. When the comet nears Earth the news make it to the TV and newspapers hardly a day before the event. And so on, and so on ...<br /><br />Neither does any kind of emotion make it to the screen. Is the genius who discovers the comet and builds the underground city a savior of humanity or a beast? The director or Dennis Hooper who is playing the role did not seem to decide until the film was done, and actually it does not make any difference because acting and directing is so confusing that you end by wondering what does this film try to say. The special effects are so cheap that not only that they cannot be convincing in the era of computer effects, but they could not have been convincing even in the 50s, four decades before this film was made.<br /><br />A total waste of time.\"',\n",
       " 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 문서를 하나씩 읽어서 반환 \"\"\"\n",
    "\n",
    "def stream_docs(path) :\n",
    "    with open(path, 'r', encoding = 'utf-8') as csv : \n",
    "        next(csv)\n",
    "        # 헤더 넘기기\n",
    "        for line in csv : \n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "\n",
    "next(stream_docs(path = 'movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 지정한 만큼 문서를 반환 \"\"\"\n",
    "\n",
    "def get_minibatch(doc_stream, size) : \n",
    "    docs, y = [], []\n",
    "    try : \n",
    "        for _ in range(size) : \n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration : \n",
    "        pass\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 데이터 종류에 상관없는 HashingVectorizer\"\"\"\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error = 'ignore',\n",
    "                        n_features = 2**21,\n",
    "                         # 해시 충돌 가능성을 줄임 => 로지스틱 회귀 모델의 가중치 개수도 늘어남\n",
    "                        preprocessor = None,\n",
    "                        tokenizer = tokenizer)\n",
    "clf = SGDClassifier(loss = 'log', random_state = 1, max_iter = 1)\n",
    "doc_stream = stream_docs(path = 'movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:20\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 외부 메모리 학습 \"\"\"\n",
    "import pyprind\n",
    "\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "for _ in range(45) : \n",
    "    X_train, y_train = get_minibatch(doc_stream, size = 1000)\n",
    "    if not X_train : \n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes = classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log', max_iter=1, random_state=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 모델 테스트 하기 \"\"\"\n",
    "\n",
    "X_test, y_test = get_minibatch(doc_stream, size = 5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('정확도: %.3f' % clf.score(X_test, y_test))\n",
    "\n",
    "clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
